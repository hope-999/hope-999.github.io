[{"title":"No.150-fastapi中异步函数怎么测试","url":"/2025/08/09/No-150-fastapi%E4%B8%AD%E5%BC%82%E6%AD%A5%E5%87%BD%E6%95%B0%E6%80%8E%E4%B9%88%E6%B5%8B%E8%AF%95/","content":"导读在 fastapi 中，经常会有异步函数，那么在测试的时候，怎么测试异步函数呢？\n需要使用专门的工具和方法。pytest 通过 pytest-asyncio 插件提供了对异步测试的支持。\n安装必要的包首先需要安装 pytest 和 pytest-asyncio：\npip install pytest pytest-asyncio\n\n基本异步测试1. 简单异步测试import pytestasync def async_function():    return 42@pytest.mark.asyncioasync def test_async_function():    result = await async_function()    assert result == 42\n\n2. 测试异步 HTTP 请求import pytestimport aiohttp@pytest.mark.asyncioasync def test_http_request():    async with aiohttp.ClientSession() as session:        async with session.get(&#x27;https://httpbin.org/get&#x27;) as resp:            assert resp.status == 200            data = await resp.json()            assert &#x27;url&#x27; in data\n\n高级用法1. 使用异步 fixtureimport pytest@pytest.fixtureasync def async_fixture():    # 异步设置代码    yield &quot;fixture value&quot;    # 异步清理代码@pytest.mark.asyncioasync def test_with_async_fixture(async_fixture):    assert async_fixture == &quot;fixture value&quot;\n\n2. 测试超时import pytestimport asyncioasync def slow_operation():    await asyncio.sleep(2)    return &quot;done&quot;@pytest.mark.asyncioasync def test_slow_operation():    with pytest.raises(asyncio.TimeoutError):        await asyncio.wait_for(slow_operation(), timeout=0.1)\n\n3. 测试异步异常import pytestasync def async_raise_exception():    raise ValueError(&quot;Something went wrong&quot;)@pytest.mark.asyncioasync def test_async_exception():    with pytest.raises(ValueError, match=&quot;Something went wrong&quot;):        await async_raise_exception()\n\n配置 pytest-asyncio可以在 pytest.ini 中配置 pytest-asyncio：\n[pytest]asyncio_mode = auto\n\n可选模式：\n\nstrict - 只运行标记为 @pytest.mark.asyncio 的测试\nauto - 自动检测异步测试函数\nlegacy - 旧版行为\n\n注意事项\n确保测试函数被 @pytest.mark.asyncio 装饰\n不要在同步函数中使用 await，这会导致语法错误\n对于复杂的异步测试，考虑使用 asyncio 的事件循环控制\n测试数据库操作时，确保使用支持异步的数据库驱动\n\n完整示例# my_async_module.pyasync def fetch_data(db):    await db.connect()    data = await db.query(&quot;SELECT * FROM table&quot;)    return data# test_my_async_module.pyimport pytestfrom unittest.mock import AsyncMockfrom my_async_module import fetch_data@pytest.mark.asyncioasync def test_fetch_data():    # 创建异步mock对象    mock_db = AsyncMock()    mock_db.query.return_value = [1, 2, 3]        result = await fetch_data(mock_db)        assert result == [1, 2, 3]    mock_db.connect.assert_awaited_once()    mock_db.query.assert_awaited_once_with(&quot;SELECT * FROM table&quot;)\n\n通过以上方法，你可以有效地测试 Python 中的异步代码。\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.147-Python生成不重复的分支名","url":"/2025/08/14/No-147-Python%E7%94%9F%E6%88%90%E4%B8%8D%E9%87%8D%E5%A4%8D%E7%9A%84%E5%88%86%E6%94%AF%E5%90%8D/","content":"导读在开发过程中，经常遇到一些名称需要自动化生成，并且名称要求唯一性。这就要求名称随时生成，并且跟前面的名称不重复。\n例如：任务id、分支id 等\n项目开发中实际上用到了生成分支名称的场景。\n介绍四种方法方法1: 使用时间戳 + 随机字符串（推荐）import timeimport randomimport stringdef generate_branch_name(prefix=&quot;feature&quot;):    timestamp = int(time.time() * 1000)    random_str = &#x27;&#x27;.join(random.choices(string.ascii_lowercase + string.digits, k=4))    return f&quot;&#123;prefix&#125;-&#123;timestamp&#125;-&#123;random_str&#125;&quot;# 示例使用print(generate_branch_name())  # 例如: feature-1751283975396-ab3dprint(generate_branch_name(&quot;bugfix&quot;))  # 例如: bugfix-1751283983855-c4e2\n\n这种方法唯一的缺点就是：并发非常高的时候，会重复。一般情况下足够了。\n方法2: 使用UUID（推荐）import uuiddef generate_branch_name(prefix=&quot;feature&quot;):    unique_id = str(uuid.uuid4())    return f&quot;&#123;prefix&#125;-&#123;unique_id&#125;&quot;# 示例使用print(generate_branch_name())  # 例如: feature-65317f15-65a8-47ea-8b5b-380c738015e0\n\n这种方式会重复吗？\nUUID v4生成规则：基于随机数生成。\n重复概率：约 1&#x2F;2^122（即约 5.3×10⁻³⁶）。\n举例：每秒生成 10 亿个 UUID，持续约 85 年 才有 50% 的概率发生一次重复。\n所以重复的概率极低。\n方法3: 基于用户和项目信息（推荐）import getpassimport timeimport hashlibdef generate_branch_name(prefix=&quot;feature&quot;, project_name=None):    user = getpass.getuser()    timestamp = int(time.time())        if project_name:        base_str = f&quot;&#123;user&#125;-&#123;project_name&#125;-&#123;timestamp&#125;&quot;    else:        base_str = f&quot;&#123;user&#125;-&#123;timestamp&#125;&quot;        # 使用hash缩短    hash_str = hashlib.md5(base_str.encode()).hexdigest()    return f&quot;&#123;prefix&#125;-&#123;hash_str&#125;&quot;# 示例使用print(generate_branch_name())  # 例如: feature-81bbf63af6cba2da8541d72c8c02fd60print(generate_branch_name(project_name=&quot;myproject&quot;))  # 例如: feature-ba34657bd9983d679d00a7d991d333ba\n\n方法4: 使用递增计数器（需要存储）(不推荐)import osdef generate_branch_name(prefix=&quot;feature&quot;):    counter_file = &quot;.branch_counter&quot;        # 读取或初始化计数器    if os.path.exists(counter_file):        with open(counter_file, &quot;r&quot;) as f:            counter = int(f.read()) + 1    else:        counter = 1        # 保存新计数器值    with open(counter_file, &quot;w&quot;) as f:        f.write(str(counter))        return f&quot;&#123;prefix&#125;-&#123;counter:04d&#125;&quot;# 示例使用print(generate_branch_name())  # 例如: feature-0001, feature-0002 等\n\n这种方式比较简单，适合并发不高的场景，一般不推荐。\n选择哪种方法取决于你的具体需求：\n\n如果只需要简单唯一性，方法1或2足够\n如果需要可读性，方法3不错\n如果需要顺序编号，使用方法4\n\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.149-fastapi中模型转json字符串的坑","url":"/2025/08/08/No-149-fastapi%E4%B8%AD%E6%A8%A1%E5%9E%8B%E8%BD%ACjson%E5%AD%97%E7%AC%A6%E4%B8%B2%E7%9A%84%E5%9D%91/","content":"导读在 fastapi 中，使用pydantic定义模型是非常简单的事情，可以让代码变得可读性更强。\n在使用的时候经常遇到 模型 -&gt; json json -&gt; 模型 之间的转换。\n在转换的时候遇到总报错，遂记录一下，谨记在心。\n模型-&gt;json报错异常先写一段正常情况的测试代码，测试工具使用pytest：\nfrom sqlmodel import Session, selectfrom app.core.db import db_enginefrom app.models.sqlmodel_table import EnvModeldef test_env_model():    with Session(db_engine) as session:        query = select(EnvModel).where(EnvModel.is_delete == 0)        result = session.exec(query).all()        print(result)\n\n测试结果：\n$ pytest -sv app/tests/scripts_service/env_model_test.pyapp/tests/scripts_service/env_model_test.py::test_env_model [EnvModel(case_env_id=&#x27;T12&#x27;, id=1, is_delete=0, updated_at=datetime.datetime(2025, 7, 10, 14, 42, 17), app_id=&#x27;&#x27;, case_env=&#x27;***内灰&#x27;, created_at=datetime.datetime(2025, 710, 14, 42, 17)), EnvModel(case_env_id=&#x27;T1760&#x27;, id=2, is_delete=0, updated_at=datetime.datetime(2025, 7, 10, 20, 3, 44), app_id=&#x27;app123456789abc&#x27;, case_env=&#x27;测试环境&#x27;, created_at=datetime.datetime(200, 14, 42, 17))]PASSED\n\n模型转json错误用法1：使用 list 类型进行转换:\ndef test_env_model():    with Session(db_engine) as session:        query = select(EnvModel).where(EnvModel.is_delete == 0)        result = session.exec(query).all()        print(result.model_dump())\n\n测试结果：\n$ pytest -sv app/tests/scripts_service/env_model_test.pyFAILED app/tests/scripts_service/env_model_test.py::test_env_model - AttributeError: &#x27;list&#x27; object has no attribute &#x27;model_dump&#x27;\n\n正确应该使用模型元素，而非列表：\ndef test_env_model():    with Session(db_engine) as session:        query = select(EnvModel).where(EnvModel.is_delete == 0)        result = session.exec(query).all()        for i in result:            print(i.model_dump())\n\n测试结果：\n$ pytest -sv app/tests/scripts_service/env_model_test.pyapp/tests/scripts_service/env_model_test.py::test_env_model &#123;&#x27;case_env_id&#x27;: &#x27;T12&#x27;, &#x27;is_delete&#x27;: 0, &#x27;case_env&#x27;: &#x27;***内灰&#x27;, &#x27;updated_at&#x27;: datetime.datetime(2025, 7, 10, 14, 42, 17), &#x27;id&#x27;: 1, &#x27;app_id&#x27;: &#x27;&#x27;, &#x27;created_at&#x27;: datetime.dateme(2025, 7, 10, 14, 42, 17)&#125;&#123;&#x27;case_env_id&#x27;: &#x27;T1760&#x27;, &#x27;is_delete&#x27;: 0, &#x27;case_env&#x27;: &#x27;测试环境&#x27;, &#x27;updated_at&#x27;: datetime.datetime(2025, 7, 10, 20, 3, 44), &#x27;id&#x27;: 2, &#x27;app_id&#x27;: &#x27;app123456789abc&#x27;, &#x27;created_at&#x27;: datetime.datetime(2025, 7, 42, 17)&#125;PASSED\n\n可以看到，被转换成dict类型了，但是日期字段还是python类型。\n打开model_dump()方法的源码看下：\ndef model_dump(    self,    *,    mode: Union[Literal[&quot;json&quot;, &quot;python&quot;], str] = &quot;python&quot;,    include: Union[IncEx, None] = None,    exclude: Union[IncEx, None] = None,    context: Union[Dict[str, Any], None] = None,    by_alias: bool = False,    exclude_unset: bool = False,    exclude_defaults: bool = False,    exclude_none: bool = False,    round_trip: bool = False,    warnings: Union[bool, Literal[&quot;none&quot;, &quot;warn&quot;, &quot;error&quot;]] = True,    serialize_as_any: bool = False,) -&gt; Dict[str, Any]:    if PYDANTIC_MINOR_VERSION &gt;= (2, 7):        extra_kwargs: Dict[str, Any] = &#123;            &quot;context&quot;: context,            &quot;serialize_as_any&quot;: serialize_as_any,        &#125;    else:        extra_kwargs = &#123;&#125;    if IS_PYDANTIC_V2:        return super().model_dump(            mode=mode,            include=include,            exclude=exclude,            by_alias=by_alias,            exclude_unset=exclude_unset,            exclude_defaults=exclude_defaults,            exclude_none=exclude_none,            round_trip=round_trip,            warnings=warnings,            **extra_kwargs,        )    else:        return super().dict(            include=include,            exclude=exclude,            by_alias=by_alias,            exclude_unset=exclude_unset,            exclude_defaults=exclude_defaults,            exclude_none=exclude_none,        )\n\n可以看到，默认的模式就是 python。\n当我们把 mode 赋值成 json 看看。\ndef test_env_model():    with Session(db_engine) as session:        query = select(EnvModel).where(EnvModel.is_delete == 0)        result = session.exec(query).all()        for i in result:            print(i.model_dump(mode=&quot;json&quot;))\n\n测试结果：\n$ pytest -sv app/tests/scripts_service/env_model_test.pyapp/tests/scripts_service/env_model_test.py::test_env_model &#123;&#x27;case_env_id&#x27;: &#x27;T12&#x27;, &#x27;is_delete&#x27;: 0, &#x27;id&#x27;: 1, &#x27;updated_at&#x27;: &#x27;2025-07-10T14:42:17&#x27;, &#x27;app_id&#x27;: &#x27;&#x27;, &#x27;case_env&#x27;: &#x27;***内灰&#x27;, &#x27;created_at&#x27;: &#x27;2025-07-10T14:42:17&#x27;&#125;&#123;&#x27;case_env_id&#x27;: &#x27;T1760&#x27;, &#x27;is_delete&#x27;: 0, &#x27;id&#x27;: 2, &#x27;updated_at&#x27;: &#x27;2025-07-10T20:03:44&#x27;, &#x27;app_id&#x27;: &#x27;app123456789abc&#x27;, &#x27;case_env&#x27;: &#x27;测试环境&#x27;, &#x27;created_at&#x27;: &#x27;2025-07-10T14:42:17&#x27;&#125;PASSED\n\n这下就是比较正常的数据了。\n总结当使用 model_dump() 进行模型转换时需要注意：\n\n要转换的类型需要是模型实例，而非 list\n需要指定 mode=&quot;json&quot;，否则日期格式无法正确转换\n\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.148-fastapi中使用标准化出入参自动化生成接口文档","url":"/2025/08/07/No-148-fastapi%E4%B8%AD%E4%BD%BF%E7%94%A8%E6%A0%87%E5%87%86%E5%8C%96%E5%87%BA%E5%85%A5%E5%8F%82%E8%87%AA%E5%8A%A8%E5%8C%96%E7%94%9F%E6%88%90%E6%8E%A5%E5%8F%A3%E6%96%87%E6%A1%A3/","content":"导读在 fastapi 中，自动化生成文档是一个非常简单的事情。\n但是也需要遵循一些非常必要的规则来帮助我们更好的生成便于阅读的文档。\n这篇文章将介绍从入参的定义、到返回结构体的定义，异常捕获几个方面来规范接口开发文档。\n使用的参数校验库为：pydantic。\n入参的定义必填参数from typing import Optional, List, Literal, Unionfrom pydantic import BaseModel, Fieldclass RemoveModuleRequest(BaseModel):    service_id: int = Field(..., description=&quot;服务 id&quot;)\n\n说明：\n\n定义 class 类\n继承 BaseModel\n使用 Field 定义参数类型等。\n这里定义了service_id字段为int类型\nField 第一个参数的默认值为 ...，就表示是必填参数\n\n非必填参数from typing import Optional, List, Literal, Unionfrom pydantic import BaseModel, Fieldclass ModuleCaseManagerCaseListRequest(BaseModel):    order_id: Optional[int] = Field(default=None, description=&quot;工单id&quot;)\n\n说明：\n\n这里定义了order_id字段为int类型\n使用 Optional 包裹住 int 表示非必填\n结合Field 第一个参数的默认值为 default=None，就表示是非必填参数\n\n注意：这里的 default=None 不能简写成 None，简写之后文档将识别不了，不知道这个问题 fastapi 官方修复了没。\n字符串参数长度定义from typing import Optional, List, Literal, Unionfrom pydantic import BaseModel, Fieldclass ModuleCaseManagerCaseListRequest(BaseModel):    env_id: Optional[str] = Field(default=None, max_length=8, min_length=4, description=&quot;工单id&quot;)\n\n说明：\n\n这里定义了env_id字段为str类型\n使用 Optional 包裹住 str 表示非必填\n结合Field 第一个参数的默认值为 default=None，就表示是非必填参数\nmax_length 定义字段最长为 8 个字符\nmin_length 定义字段最短为 4 个字符\n\n参数可选值class RequestEnv(BaseModel):    env: Literal[&quot;dev&quot;, &quot;test&quot;, &quot;pro&quot;] = Field(        ..., description=&quot;环境: dev-开发, test-测试, pro-现网&quot;    )\n\n说明：\n\nLiteral[&quot;dev&quot;, &quot;test&quot;, &quot;pro&quot;] 定义可选值\n\n嵌套参数以上处理的都是层级比较单一的入参例如格式这样的:\n &#123;    &quot;service&quot;: &quot;&quot;,    &quot;plan_id&quot;: &quot;0&quot;,    &quot;order_id&quot;: &quot;0&quot;,    &quot;env_id&quot;: &quot;&quot;&#125;\n\n如果要处理多个层级的入参，例如这样的参数怎么处理:\n  &#123;    &quot;services&quot;: [&#123;        &quot;service_type&quot;: &quot;&quot;,        &quot;service_info_id&quot;: 0,        &quot;branch_id&quot;: 0,        &quot;api_ids&quot;: [101, 201, 345]    &#125;],    &quot;plan_id&quot;: &quot;0&quot;,    &quot;order_id&quot;: &quot;0&quot;,    &quot;env_id&quot;: &quot;&quot;&#125;\n\n 只需要再定义一个模型，进行嵌套即可：\n class ApiCaseManagerRunCaseServiceData(BaseModel):    service_type: str = Field(..., description=&quot;服务类型&quot;)    service_info_id: int = Field(..., description=&quot;服务id&quot;)    branch_id: int = Field(..., description=&quot;分支id&quot;)    api_ids: list = Field(..., description=&quot;接口id列表&quot;, example=[101, 201, 345])class ApiCaseManagerRunCaseRequest(BaseModel):    services: List[ApiCaseManagerRunCaseServiceData] = Field(..., description=&quot;服务列表&quot;) # 这里嵌套模型    plan_id: Optional[int] = Field(default=0, description=&quot;计划 ID，关联业务计划的标识&quot;)    order_id: Optional[int] = Field(default=0, description=&quot;工单id&quot;)    env_id: str = Field(..., description=&quot;环境id&quot;)\n\n返回结构体定义基础结构体的定义T = TypeVar(&quot;T&quot;)  # 泛型类型变量，用于data字段class BaseResponse(BaseModel, Generic[T]):    &quot;&quot;&quot;    基础响应模型，所有API响应都应继承此模型    &quot;&quot;&quot;    code: int = Field(default=0, description=&quot;状态码，0表示成功，非0表示错误&quot;, example=0)    rec_ts: float = Field(        ...,        description=&quot;请求接收时间戳（Unix时间戳，精确到毫秒）&quot;,        example=1611399999.999999,    )    rsp_ts: float = Field(        ...,        description=&quot;响应生成时间戳（Unix时间戳，精确到毫秒）&quot;,        example=1611399999.999999,    )    msg: str = Field(        default=&quot;success&quot;,        description=&quot;状态消息，成功时为&#x27;success&#x27;，错误时为错误描述&quot;,        example=&quot;success&quot;,    )    data: Optional[T] = Field(default=None, description=&quot;实际返回的数据内容，错误时为null&quot;)    @classmethod    def success(cls, data: T = None, msg: str = &quot;success&quot;):        &quot;&quot;&quot;成功的快捷方法&quot;&quot;&quot;        now = datetime.now().timestamp()        return cls(code=0, rec_ts=now, rsp_ts=now, msg=msg, data=data)\n\n定义要返回的数据模型class PeopleDataModel(BaseModel):    numbers: List[int]    people: List[str]class PeopleDataModelResponse(BaseResponse[PeopleDataModel]):    pass\n\n路由函数中使用@router.post(&quot;/case_list/1.0.0&quot;, response_model=PeopleDataModelResponse, name=&quot;用例列表&quot;)def case_list(    db_session: DbSessionDep    request: ApiCaseManagerCaseListRequest) -&gt; Any:    &quot;&quot;&quot;用例列表    Args:        db_session: 数据库连接        request: 请求参数    Returns:        Any    &quot;&quot;&quot;    # 获取查询结果（列表）    data = case_list_crud(request, db_session)    return PeopleDataModelResponse.success(data=data)\n\n返回数据格式示例&#123;  &quot;code&quot;: 0,  &quot;rec_ts&quot;: 1611399999.999999,  &quot;rsp_ts&quot;: 1611399999.999999,  &quot;msg&quot;: &quot;success&quot;,  &quot;data&quot;:&#123;      &quot;numbers&quot;: 0,      &quot;people&quot;: &quot; 小明&quot;    &#125;&#125;\n\n也可以不用定义PeopleDataModelResponse@router.post(&quot;/case_list/1.0.0&quot;, response_model=BaseResponse[PeopleDataModel], name=&quot;用例列表&quot;)def case_list(    db_session: DbSessionDep    request: ApiCaseManagerCaseListRequest) -&gt; Any:    &quot;&quot;&quot;用例列表    Args:        db_session: 数据库连接        request: 请求参数    Returns:        Any    &quot;&quot;&quot;    # 获取查询结果（列表）    data = case_list_crud(request, db_session)    return BaseResponse[PeopleDataModel].success(data=data)\n\n如果返回的数据data是列表from typing import List@router.post(&quot;/case_list/1.0.0&quot;, response_model=BaseResponse[List[PeopleDataModel]], name=&quot;用例列表&quot;)def case_list(    db_session: DbSessionDep    request: ApiCaseManagerCaseListRequest) -&gt; Any:    &quot;&quot;&quot;用例列表    Args:        db_session: 数据库连接        request: 请求参数    Returns:        Any    &quot;&quot;&quot;    # 获取查询结果（列表）    data = case_list_crud(request, db_session) # 返回的列表中数据的类型是 PeopleDataModel    return BaseResponse[List[PeopleDataModel]].success(data=data)\n\n返回数据格式示例&#123;  &quot;code&quot;: 0,  &quot;rec_ts&quot;: 1611399999.999999,  &quot;rsp_ts&quot;: 1611399999.999999,  &quot;msg&quot;: &quot;success&quot;,  &quot;data&quot;:[    &#123;    &quot;numbers&quot;: 0,    &quot;people&quot;: &quot; 小明&quot;    &#125;,    &#123;    &quot;numbers&quot;: 2,    &quot;people&quot;: &quot; 小蘑菇&quot;    &#125;  ]&#125;\n\n返回体结构灵感来源：https://docs.pydantic.dev/2.8/concepts/models/#generic-models 中的 Generic models 章节。\n异常捕获定义异常类class ResponseErrorV2(Exception):    def __init__(self, status_code: int, code: int, msg: str):        self.status_code = status_code        self.code = code        self.msg = msg\n\nfastapi捕获异常main.py 入口函数：\n@app.exception_handler(ResponseErrorV2)async def custom_error_handler(request: Request, exc: ResponseErrorV2):    return JSONResponse(        status_code=exc.status_code,        content=&#123;&quot;code&quot;: exc.code, &quot;msg&quot;: exc.msg&#125;,    )\n\n主动抛异常raise ResponseErrorV2(status_code=400, code=400001, msg=&quot;请求头必须携带Token&quot;)\n\n接口返回数据结构status: 400\n&#123;  &quot;code&quot;: 400001,  &quot;msg&quot;: &quot;请求头必须携带Token&quot;&#125;\n\n\n生成文档启动fastapi服务，编写shell脚本：generate_swagger.sh\n#! /bin/bashcurl -X GET http://127.0.0.1:9000/openapi.json &gt; docs/api/swagger.json\n\n分别执行如下命令：\nchmod +x generate_swagger.sh./generate_swagger.sh\n\n总结pydantic 不仅可以在编写代码的过程中规范书写格式，还能自动化生成文档，还可以帮我们校验出入参的格式，让代码的可读性进一步加强。\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.152-fastapi中如何写webhook接口","url":"/2025/08/11/No-152-fastapi%E4%B8%AD%E5%A6%82%E4%BD%95%E5%86%99webhook%E6%8E%A5%E5%8F%A3/","content":"导读在构建 CICD 的过程中，经常会用到一些工具，比如：Jenkins、GitLab、Coding 等。\n这些构建工具都有一个共同点：它们能支持 webHook 的触发。\n比如 gitlib 中设置当前 push 代码的时候触发 webhook。\n当我们配置了 webhook 以后，会在触发事件发生的时候，去请求一个自定义的 url。\n这样就丰富了我们在构建 CICD 的流程中，去处理更多的业务逻辑。\nwebhook 如何自定义接口@router.post(&quot;/webhook/1.0.0&quot;, name=&quot;webhook&quot;)async def add_new(    request: Request,) -&gt; Any:    # 解析 Webhook 请求体的 JSON 数据    body = await request.body()    # 解析 JSON 数据    try:        data = json.loads(body.decode(&quot;utf-8&quot;))    except json.JSONDecodeError:        return Response(&quot;ok&quot;)    logger.info(f&quot;webhook的body=&#123;data&#125;&quot;)    # 这里添加业务逻辑    # ...    return Response(&quot;ok&quot;)\n\n说明\n请求方法定义为 post，一般要使用post方法。\n设置请求类型为 application&#x2F;json\n读取请求数据，并解析。body = await request.body()\n读取 json 数据,data = json.loads(body.decode(&quot;utf-8&quot;))，这里要注意异常的捕获，防止报错。\n在处理逻辑中，一般需要进行数据校验，避免接口报错。\n在需要使用日志信息时，要添加 logger。\n解析 body 中的参数。\n添加业务处理逻辑。\n返回 ok, 返回 ok 告诉 webhook 通知发送端，接口正常处理完成。这里要记住一定要返回 ok，否则会不停的触发事件。return Response(&quot;ok&quot;)\n\n如何测试？使用 curl 的 post 方式，带上 body 数据，示例：\ncurl --location --request POST &#x27;http://127.0.0.1:9000/webhook/1.0.0&#x27; \\--header &#x27;Content-Type: application/json&#x27; \\--data-raw &#x27;&#123;&quot;object_kind&quot;: &quot;deployment&quot;, &quot;id&quot;: 123&#125;&#x27;\n\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.151-fastapi中批量更新数据怎么保证数据的一致性","url":"/2025/08/10/No-151-fastapi%E4%B8%AD%E6%89%B9%E9%87%8F%E6%9B%B4%E6%96%B0%E6%95%B0%E6%8D%AE%E6%80%8E%E4%B9%88%E4%BF%9D%E8%AF%81%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7/","content":"导读在 fastapi 中，API 最常见的情况是批量更新数据，一个逻辑层的函数可能包含多次数据库交互，有新增数据、更新数据逻辑，那么在保证数据的一致性方面应该怎么做呢？\n下面我将详细说明在 FastAPI 中实现批量更新的两种主要方式，并提供完整的代码示例。\n1. 使用数据库事务实现批量更新这种方式通过数据库事务保证所有更新操作要么全部成功，要么全部失败。\n完整示例代码from fastapi import FastAPI, HTTPException, Dependsfrom pydantic import BaseModelfrom typing import Listfrom sqlalchemy import create_engine, Column, Integer, Stringfrom sqlalchemy.ext.declarative import declarative_basefrom sqlalchemy.orm import sessionmaker, Session# 数据库配置SQLALCHEMY_DATABASE_URL = &quot;sqlite:///./test.db&quot;engine = create_engine(SQLALCHEMY_DATABASE_URL)SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)Base = declarative_base()# 数据库模型class User(Base):    __tablename__ = &quot;users&quot;    id = Column(Integer, primary_key=True, index=True)    name = Column(String(50))    email = Column(String(50))    status = Column(String(20))# 创建表Base.metadata.create_all(bind=engine)# Pydantic 模型class UserUpdate(BaseModel):    id: int    name: str = None    email: str = None    status: str = Noneapp = FastAPI()# 依赖项 - 获取数据库会话def get_db():    db = SessionLocal()    try:        yield db    finally:        db.close()@app.put(&quot;/users/bulk-update-transaction&quot;)async def bulk_update_users_transaction(    users: List[UserUpdate],     db: Session = Depends(get_db)):    try:        # 开始事务        db.begin()                for user_data in users:            # 查询用户            user = db.query(User).filter(User.id == user_data.id).first()            if not user:                raise HTTPException(status_code=404, detail=f&quot;User with id &#123;user_data.id&#125; not found&quot;)                        # 更新字段            if user_data.name is not None:                user.name = user_data.name            if user_data.email is not None:                user.email = user_data.email            if user_data.status is not None:                user.status = user_data.status                        # 也可以使用 merge 方法            # db.merge(user)                # 提交事务        db.commit()    except HTTPException:        # 已知异常直接抛出        db.rollback()        raise    except Exception as e:        # 其他异常回滚并返回错误        db.rollback()        raise HTTPException(status_code=500, detail=str(e))        return &#123;&quot;message&quot;: &quot;Batch update completed successfully&quot;, &quot;updated_count&quot;: len(users)&#125;\n\n关键点说明\n事务管理：\n\ndb.begin() 显式开始事务\ndb.commit() 提交事务\ndb.rollback() 在异常时回滚\n\n\n错误处理：\n\n处理了用户不存在的场景\n捕获所有异常确保事务回滚\n返回适当的HTTP状态码\n\n\n原子性保证：\n\n所有更新要么全部成功，要么全部失败\n中间出错不会导致部分更新\n\n\n\n2. 使用批量操作语句实现批量更新这种方式通过单个SQL语句执行批量更新，效率更高。\n完整示例代码2from sqlalchemy import update, bindparam@app.put(&quot;/users/bulk-update-bulk-statement&quot;)async def bulk_update_users_bulk_statement(    users: List[UserUpdate],     db: Session = Depends(get_db)):    try:        # 构建批量更新语句        stmt = (            update(User)            .where(User.id == bindparam(&#x27;user_id&#x27;))            .values(&#123;                User.name: bindparam(&#x27;user_name&#x27;),                User.email: bindparam(&#x27;user_email&#x27;),                User.status: bindparam(&#x27;user_status&#x27;)            &#125;)        )                # 准备参数列表        params = []        for user in users:            param = &#123;                &#x27;user_id&#x27;: user.id,                &#x27;user_name&#x27;: user.name,                &#x27;user_email&#x27;: user.email,                &#x27;user_status&#x27;: user.status            &#125;            params.append(param)                # 执行批量更新        result = db.execute(stmt, params)        db.commit()                # 返回更新的行数        updated_count = result.rowcount            except Exception as e:        db.rollback()        raise HTTPException(status_code=500, detail=str(e))        return &#123;        &quot;message&quot;: &quot;Batch update completed with bulk statement&quot;,        &quot;updated_count&quot;: updated_count    &#125;\n\n关键点说明2\n批量SQL构建：\n\n使用 update() 和 bindparam() 构建参数化查询\n单个SQL语句处理所有更新\n\n\n参数准备：\n\n将输入数据转换为参数列表\n每个参数对应一个字典\n\n\n执行效率：\n\n相比循环更新，减少数据库往返次数\n数据库可以优化执行计划\n\n\n返回值：\n\n通过 result.rowcount 获取实际更新的行数\n\n\n\n两种方式的比较\n\n\n特性\n事务方式\n批量语句方式\n\n\n\n实现复杂度\n简单直接\n需要构建SQL语句\n\n\n性能\n中等（多次数据库调用）\n高（单次数据库调用）\n\n\n灵活性\n高（可处理复杂逻辑）\n中（适合简单字段更新）\n\n\n错误处理\n可以逐条检查\n批量处理，难以单独处理某条失败\n\n\n适用场景\n需要复杂逻辑或验证的更新\n大批量简单字段更新\n\n\n测试示例你可以使用这样的请求体测试上述API：\n[    &#123;        &quot;id&quot;: 1,        &quot;name&quot;: &quot;New Name 1&quot;,        &quot;email&quot;: &quot;new1@example.com&quot;,        &quot;status&quot;: &quot;active&quot;    &#125;,    &#123;        &quot;id&quot;: 2,        &quot;name&quot;: &quot;New Name 2&quot;,        &quot;email&quot;: &quot;new2@example.com&quot;,        &quot;status&quot;: &quot;inactive&quot;    &#125;]\n\n实际业务中，根据你的具体需求选择合适的实现方式。\n\n对于需要复杂业务逻辑的更新，事务方式更合适；\n对于纯粹的大批量数据更新，批量语句方式性能更好。\n\n总之：在一个逻辑函数中处理事务，避免中间状态，确保原子性和一致性，尽量不要写一段逻辑提交一次，而是在逻辑函数结束时候提交或回滚一次。如果遇到高并发场景，还需要考虑锁策略。\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.154-如何在fastapi中使用BackgroundTasks","url":"/2025/08/12/No-154-%E5%A6%82%E4%BD%95%E5%9C%A8fastapi%E4%B8%AD%E4%BD%BF%E7%94%A8BackgroundTasks/","content":"导读FastAPI 的 BackgroundTasks 功能允许你将任务添加到后台执行，这样可以在响应返回给客户端后继续处理耗时操作。\n常用的场景有发送邮件，发送短信等。\n这个功能尽量不要使用在生产环境，如果功能是边缘业务，倒是可以使用，简单方便。它的缺点就是出了问题，很难定位问题所在。需要添加详细的日志信息。\n以下是使用 BackgroundTasks 的详细指南：\n基本用法\n首先导入 BackgroundTasks：\n\nfrom fastapi import BackgroundTasks, FastAPI\n\n\n创建 FastAPI 应用并定义后台任务函数：\n\napp = FastAPI()def write_log(message: str):    with open(&quot;log.txt&quot;, mode=&quot;a&quot;) as log:        log.write(message)def send_email(email: str, message: str):    # 模拟发送电子邮件    print(f&quot;发送邮件到 &#123;email&#125;: &#123;message&#125;&quot;)\n\n\n在路径操作中使用 BackgroundTasks：\n\n@app.post(&quot;/send-notification/&#123;email&#125;&quot;)async def send_notification(email: str, background_tasks: BackgroundTasks):    background_tasks.add_task(send_email, email, message=&quot;这是一条通知&quot;)    background_tasks.add_task(write_log, f&quot;发送通知到 &#123;email&#125;&quot;)    return &#123;&quot;message&quot;: &quot;通知已发送&quot;&#125;\n\n高级用法1. 依赖注入你可以将 BackgroundTasks 作为依赖项注入：\nfrom fastapi import Dependsdef get_background_tasks(background_tasks: BackgroundTasks = BackgroundTasks()):    return background_tasks@app.post(&quot;/another-route&quot;)async def another_route(background_tasks: BackgroundTasks = Depends(get_background_tasks)):    background_tasks.add_task(some_task)    return &#123;&quot;message&quot;: &quot;任务已添加&quot;&#125;\n\n2. 与路径操作函数一起使用后台任务可以与常规路径操作函数一起使用：\n@app.post(&quot;/process-data&quot;)async def process_data(    data: dict,    background_tasks: BackgroundTasks):    # 立即处理一些数据    processed_data = &#123;k: v.upper() for k, v in data.items()&#125;        # 添加后台任务    background_tasks.add_task(store_data_in_db, processed_data)        return &#123;&quot;processed_data&quot;: processed_data&#125;\n\n3. 类方法作为后台任务你也可以使用类方法作为后台任务：\nclass NotificationService:    @classmethod    def send_email(cls, email: str, message: str):        # 发送邮件逻辑        pass@app.post(&quot;/send-email&quot;)async def send_email_route(    email: str,     background_tasks: BackgroundTasks):    background_tasks.add_task(        NotificationService.send_email,        email,        message=&quot;Hello from FastAPI&quot;    )    return &#123;&quot;status&quot;: &quot;Email will be sent&quot;&#125;\n\n注意事项\n任务顺序：添加任务的顺序就是它们执行的顺序。\n\n异常处理：后台任务中的异常不会传播到主请求，需要自行处理。\n\n测试：在测试时，可以使用 BackgroundTasks 的 tasks 属性来检查添加的任务：\n\n\ndef test_background_task():    background_tasks = BackgroundTasks()    test_app.add_task(some_task)    assert len(background_tasks.tasks) == 1\n\n\n长时间运行任务：对于非常耗时的任务，考虑使用 Celery 或其他任务队列。\n\n依赖项：后台任务函数可以接受 FastAPI 的依赖项。\n\n\n完整示例from fastapi import FastAPI, BackgroundTasksimport timeapp = FastAPI()def process_data_in_background(data: dict):    # 模拟耗时操作    time.sleep(5)    with open(&quot;processed_data.txt&quot;, &quot;a&quot;) as f:        f.write(f&quot;&#123;data&#125;\\n&quot;)@app.post(&quot;/process/&quot;)async def process_data(data: dict, background_tasks: BackgroundTasks):    background_tasks.add_task(process_data_in_background, data)    return &#123;&quot;message&quot;: &quot;数据正在后台处理&quot;, &quot;data&quot;: data&#125;\n\n通过这种方式，你可以轻松地将耗时操作放到后台执行，同时立即返回响应给客户端。\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.155-fastapi项目中Dockerfile怎么写","url":"/2025/08/14/No-155-fastapi%E9%A1%B9%E7%9B%AE%E4%B8%ADDockerfile%E6%80%8E%E4%B9%88%E5%86%99/","content":"导读fastapi 项目代码写完了，怎么打包部署？\n部署到 k8s 集群中，Dockerfile 怎么写呢？\n以下是针对 FastAPI 项目的 生产级 Dockerfile 示例，包含性能优化、安全实践和 Kubernetes 适配建议，支持异步请求和高并发场景：\n优化版 Dockerfile（支持 ASGI + 多阶段构建）# 阶段1：构建依赖FROM python:3.11-slim as builderWORKDIR /appENV PYTHONUNBUFFERED=1 \\    PIP_NO_CACHE_DIR=1 \\    PIP_DISABLE_PIP_VERSION_CHECK=1# 安装系统依赖（根据项目需求调整）RUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\    gcc python3-dev &amp;&amp; \\    rm -rf /var/lib/apt/lists/*# 安装Python依赖（分层优化，利用Docker缓存）COPY requirements.txt .RUN pip install --user --no-warn-script-location -r requirements.txt# ---# 阶段2：生产镜像FROM python:3.11-slimWORKDIR /app# 从builder阶段复制已安装的Python包COPY --from=builder /root/.local /root/.local# 复制项目代码（通过.dockerignore过滤无关文件）COPY . .# 安全配置：使用非root用户RUN useradd -m appuser &amp;&amp; chown -R appuser /appUSER appuser# 环境变量ENV PATH=/home/appuser/.local/bin:$PATH \\    PYTHONUNBUFFERED=1 \\    PYTHONPATH=/app \\    UVICORN_HOST=0.0.0.0 \\    UVICORN_PORT=8000# 暴露端口EXPOSE 8000# 健康检查端点（需在FastAPI中实现/health）HEALTHCHECK --interval=30s --timeout=3s \\  CMD curl -f http://localhost:8000/health || exit 1# 启动命令（推荐使用Uvicorn + Gunicorn组合）CMD [&quot;gunicorn&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;--bind&quot;, &quot;0.0.0.0:8000&quot;, &quot;--workers&quot;, &quot;4&quot;, &quot;app.main:app&quot;]\n\n\n关键优化点解析\nASGI 服务器选择  \n\n使用 UvicornWorker + Gunicorn 组合：\nUvicorn 处理异步请求\nGunicorn 管理进程（建议 worker 数 &#x3D; min(2*CPU核心+1, 8)）\n\n\n\n\n安全增强  \n\n禁用 root 用户运行容器\n通过 .dockerignore 排除 __pycache__、.env 等敏感文件\n使用 --no-warn-script-location 避免权限警告\n\n\nKubernetes 适配  \n\n健康检查：在 Deployment 中配置：\nlivenessProbe:  httpGet:    path: /health    port: 8000\n\n资源限制：建议在 K8s 中设置 CPU&#x2F;Memory 限制\n\n环境变量：敏感配置通过 K8s Secrets 注入\n\n\n\n性能优化  \n\n多阶段构建减少镜像体积（最终镜像约 150MB）\n依赖分层安装（修改代码时不会重新安装依赖）\n\n\n\n\n不同场景的启动命令调整\n\n\n场景\nCMD 指令示例\n\n\n\n纯开发环境\nCMD [&quot;uvicorn&quot;, &quot;app.main:app&quot;, &quot;--reload&quot;, &quot;--host&quot;, &quot;0.0.0.0&quot;]\n\n\n高并发生产环境\nCMD [&quot;gunicorn&quot;, &quot;-k&quot;, &quot;uvicorn.workers.UvicornWorker&quot;, &quot;--bind&quot;, &quot;...&quot;]\n\n\n单节点测试\nCMD [&quot;uvicorn&quot;, &quot;app.main:app&quot;, &quot;--workers&quot;, &quot;4&quot;]\n\n\n\n配套的 requirements.txt 建议fastapi==0.109.0uvicorn==0.27.0gunicorn==21.2.0# 其他依赖...\n\n\n常见问题解决方案\n静态文件处理添加 Nginx 容器或通过 Kubernetes Ingress 配置：\nRUN pip install aiofiles  # 用于异步静态文件处理\n\nAlpine 镜像问题如需更小镜像，可替换基础镜像为 python:3.11-alpine，但需注意：\nRUN apk add --no-cache gcc musl-dev  # Alpine版gcc\n\n超时控制在 Gunicorn 中增加参数：\nCMD [&quot;gunicorn&quot;, &quot;--timeout&quot;, &quot;120&quot;, &quot;...&quot;]\n\n简版 Dockerfile (直接使用fastapi命令启动)（快速启动推荐使用）FROM python:3.12LABEL authors=&quot;&lt;your name&gt;&quot;WORKDIR /code# RUN apt-get update -y &amp;&amp; apt-get install -y python3-pip &amp;&amp; pip3 install pip --upgradeCOPY requirements.txt requirements.txtRUN pip3 install --no-cache-dir -r requirements.txt -i https://mirrors.aliyun.com/pypi/simple/COPY . .# 批量设置执行权限RUN find . -name &quot;*.sh&quot; -exec chmod +x &#123;&#125; \\;CMD [&quot;./run_main.sh&quot;]\n\nrun_main.sh:\n#! /bin/bashexport PYTHONPATH=/code:$PYTHONPATHfastapi run --port 9000\n\n本地开发环境，run_dev.sh的启动脚本：\n#! /bin/bashexport PYTHONPATH=/code:$PYTHONPATHfastapi dev --port 9000 --reload\n\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.156-fastapi中健康检查接口怎么写","url":"/2025/08/18/No-156-fastapi%E4%B8%AD%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5%E6%8E%A5%E5%8F%A3%E6%80%8E%E4%B9%88%E5%86%99/","content":"导读fastapi 项目代码写完了，部署的时候，运维同学要求要有 health 健康检查接口，怎么办？\n基础实现（简单版）from fastapi import FastAPI, APIRouterapp = FastAPI()# 健康检查端点@app.get(&quot;/health&quot;)async def health_check():    return &#123;&quot;status&quot;: &quot;ok&quot;, &quot;message&quot;: &quot;Service is healthy&quot;&#125;\n\n高级实现（带核心依赖检查）from fastapi import FastAPI, APIRouter, Depends, HTTPExceptionfrom sqlalchemy import textfrom redis import Redisfrom .database import SessionLocal  # 假设你有一个数据库会话app = FastAPI()# 依赖项示例def get_db():    db = SessionLocal()    try:        yield db    finally:        db.close()# Redis 连接检查def check_redis():    try:        redis = Redis(host=&quot;localhost&quot;, port=6379, db=0)        return redis.ping()    except Exception:        return False# 完整健康检查@app.get(&quot;/health&quot;)async def health_check(db: SessionLocal = Depends(get_db)):    checks = &#123;        &quot;database&quot;: False,        &quot;redis&quot;: False,        &quot;service&quot;: True    &#125;        # 数据库检查    try:        db.execute(text(&quot;SELECT 1&quot;))        checks[&quot;database&quot;] = True    except Exception as e:        pass        # Redis检查    checks[&quot;redis&quot;] = check_redis()        # 综合状态    overall_status = all(checks.values())        if not overall_status:        raise HTTPException(            status_code=503,            detail=&quot;Service Unavailable&quot;,            headers=&#123;&quot;Retry-After&quot;: &quot;10&quot;&#125;        )        return &#123;        &quot;status&quot;: &quot;ok&quot; if overall_status else &quot;degraded&quot;,        &quot;checks&quot;: checks    &#125;\n\n使用 APIRouter（推荐用于模块化）# health_router.pyfrom fastapi import APIRouter, Dependsfrom .dependencies import get_db  # 导入你的依赖项router = APIRouter(tags=[&quot;Monitoring&quot;])@router.get(&quot;/health&quot;, summary=&quot;服务健康检查&quot;, response_description=&quot;返回服务状态&quot;)async def health_check(db = Depends(get_db)):    # 实现检查逻辑...    return &#123;&quot;status&quot;: &quot;ok&quot;&#125;# main.pyfrom fastapi import FastAPIfrom .routers import health_routerapp = FastAPI()app.include_router(health_router.router)\n\n关键配置说明\n端点选择：\n\n/health：行业标准\n/healthz：Kubernetes 常用\n/ping：简单探测\n\n\n响应设计：\n&#123;  &quot;status&quot;: &quot;ok&quot;,  // 或 &quot;error&quot;/&quot;warning&quot;  &quot;version&quot;: &quot;1.0.0&quot;,  &quot;dependencies&quot;: &#123;    &quot;database&quot;: true,    &quot;cache&quot;: false  &#125;,  &quot;uptime&quot;: 12345.67&#125;\n\nHTTP状态码：\n\n200 OK：所有系统正常\n503 Service Unavailable：关键服务不可用\n\n\n性能优化：\n\n添加缓存（避免频繁检查数据库）\n设置超时（防止健康检查阻塞）\n\nfrom fastapi import BackgroundTasks@app.get(&quot;/health&quot;)async def health_check(background_tasks: BackgroundTasks):    # 将耗时检查放入后台    background_tasks.add_task(check_database)    return &#123;&quot;status&quot;: &quot;checking_in_background&quot;&#125;\n\n生产环境最佳实践\n添加安全保护：\nfrom fastapi.security import APIKeyHeaderAPI_KEY_NAME = &quot;X-HEALTH-KEY&quot;api_key_scheme = APIKeyHeader(name=API_KEY_NAME)@app.get(&quot;/health&quot;)async def secured_health(key: str = Depends(api_key_scheme)):    if key != &quot;SECRET_KEY&quot;:        raise HTTPException(status_code=403)    return &#123;&quot;status&quot;: &quot;ok&quot;&#125;\n\n集成 Prometheus 监控：\nfrom prometheus_fastapi_instrumentator import InstrumentatorInstrumentator().instrument(app).expose(app)\n\n添加版本信息：\nimport importlib.metadata@app.get(&quot;/health&quot;)async def health_check():    return &#123;        &quot;version&quot;: importlib.metadata.version(&quot;your-package&quot;),        &quot;dependencies&quot;: []    &#125;\n\n测试方法# test_health.pyfrom fastapi.testclient import TestClientfrom main import appclient = TestClient(app)def test_health_check():    response = client.get(&quot;/health&quot;)    assert response.status_code == 200    assert response.json()[&quot;status&quot;] == &quot;ok&quot;\n\n部署注意事项\n在 Kubernetes 中配置：\nlivenessProbe:  httpGet:    path: /health    port: 8000  initialDelaySeconds: 5  periodSeconds: 10readinessProbe:  httpGet:    path: /health    port: 8000  initialDelaySeconds: 30  periodSeconds: 5\n\n在负载均衡器（如 Nginx）中配置：\nlocation = /health &#123;    access_log off;    proxy_pass http://backend;&#125;\n\n这样实现的健康检查接口既满足基本需求，又能适应生产环境的复杂要求，同时保持代码的可维护性和扩展性。\n总结\n场景比较简单：直接使用基础版即可\n场景相对复杂：可考虑使用 FastAPI Router 分割模块\n需要校验 Header 需，添加 APIKey 的保护：使用 APIKeyHeader 中间件保护接口\n为了方便运维和监控：集成 Prometheus 并添加版本信息等元数据\n\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.159-fastapi中数据库隔离级别的配置的使用","url":"/2025/09/15/No-159-fastapi%E4%B8%AD%E6%95%B0%E6%8D%AE%E5%BA%93%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB%E7%9A%84%E9%85%8D%E7%BD%AE%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"导读在做一次数据库插入数据的时候，发现重复插入问题，经过排查发现是并发导致。于是开始排查原因。\n问题原因排查经过采用分布式锁使用了分布式锁对操作数据库逻辑进行了加锁，但是发现还是会有重复插入问题。\n发现每次插入数据前都会查询一次数据，如果没有就插入，有就跳过。\n添加调试日志加了锁还是不能解决问题。于是将操作数据库前后的日志打印出来。\n发现当两个请求同时查询和插入的时候，明明已经加锁了，但是数据插入没成功，虽然每次请求都显式进行了 commit 操作。\n所以结果就是，两个请求都查询了数据库，发现没有数据，然后都插入了数据，导致重复插入。\n解决方案经过一番搜索，发现：如果你用的数据库（比如 MySQL 默认 InnoDB），事务隔离级别是 REPEATABLE READ。线程 A 插入并提交，线程 B 在插入前就开启了事务，那么它的快照还是旧的，导致查不到最新提交的数据。\n所以，解决方案就是：将数据库隔离级别改为 READ COMMITTED。\n具体 fastapi 中操作在创建数据库引擎的时候，添加 isolation_level=&quot;READ COMMITTED&quot; 参数即可。\nfrom sqlmodel import Session, create_enginefrom settings import settingsengine_o_db_ex_&lt;db_name&gt; = create_engine(    str(settings.DB_O_DB_EX_&lt;DB_NAME&gt;_URI),    pool_recycle=300,    pool_pre_ping=True,    pool_size=10,  # 连接池大小    max_overflow=20,  # 溢出连接数    isolation_level=&quot;READ COMMITTED&quot;, # 设置隔离级别)def get_db_o_ex_&lt;db_name&gt;() -&gt; Generator[Session, None, None]:    try:        with Session(engine_o_db_ex_&lt;db_name&gt;) as session:            yield session    except SQLAlchemyError as e:        # 可选择加入日志记录或错误上报        logger.exception(f&quot;Error occurred while managing session: &#123;e&#125;&quot;)        raiseDBExSessionDep = Annotated[Session, Depends(get_db_o_ex_&lt;db_name&gt;)]\n\n再使用依赖注入的方式，统一获取 Session 对象，这样在跨 Session 的情况下，就可以避免重复插入了。\n总结数据库隔离级别，这个参数在平时开发中，很少用到，但是当遇到并发问题的时候，这个参数就派上用场了。\n遇到问题，不要慌，先排查原因，可以使用 debug 模式，打印日志，一步步排查。\nMySQL 支持以下隔离级别（SQLAlchemy 里字符串要写全大写）：\n\n“READ UNCOMMITTED”\n“READ COMMITTED” ✅ 推荐，能看到别的事务已提交的数据\n“REPEATABLE READ” （MySQL 默认，可能导致你查不到刚提交的数据）\n“SERIALIZABLE”\n\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.161-fastapi中分布式锁的使用","url":"/2025/09/28/No-161-fastapi%E4%B8%AD%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"🚀 FastAPI 中分布式锁的使用：从一次“库存超卖”说起去年双 11，我接到一个临时需求：某个电商项目要在 FastAPI 微服务中接入一个限时秒杀活动。听起来很简单：下单时，先扣减库存，再生成订单。\n但活动刚开始 5 分钟，测试环境里就出现了库存被超卖的问题。\n我一脸问号：\n\n“不是加了数据库行级锁吗？怎么还会超卖？”\n\n后来排查发现：\n\nFastAPI 部署在 多实例 + 多进程 模式下。\n请求并发打到不同节点，单机锁根本没法控制全局并发。\n导致库存还没刷新，另一个节点也以为还能买。\n\n这时，我们才意识到：👉 需要一把分布式锁。\n\n🔑 为什么要分布式锁？常见场景：\n\n秒杀&#x2F;抢购：防止库存被多减\n任务调度：避免多个副本同时跑同一个任务\n接口幂等：防止重复请求造成数据重复写入\n\n在 单机环境 下，threading.Lock 就够了。但在 分布式环境（Kubernetes &#x2F; 多副本部署）下，必须要靠 Redis 这种 中心化存储 来协调。\n\n🧩 分布式锁的原理分布式锁的基本原理很简单：\n\n加锁：尝试在 Redis 设置一个 Key（只允许第一个写入成功）。\n执行业务逻辑：执行库存扣减、报表生成等。\n解锁：执行完后删除 Key，但要注意不能误删别人的锁。\n\nRedis 的 SETNX + 过期时间（EXPIRE）就能实现这一过程。\n\n⚡ 在 FastAPI 中的实战示例下面分享一个真实的业务改造案例：\nfrom contextlib import contextmanagerfrom redis import Redisfrom redis.lock import Lockimport logginglogger = logging.getLogger(__name__)redis_client = Redis(host=&quot;127.0.0.1&quot;, port=6379, decode_responses=True)@contextmanagerdef get_redis_lock(lock_key: str, timeout: int = 10):    &quot;&quot;&quot;    获取 Redis 分布式锁（上下文管理器方式）    Args:        lock_key (str): 锁的 key        timeout (int, optional): 过期时间，秒. Defaults to 10.    &quot;&quot;&quot;    lock = Lock(        redis=redis_client,        name=lock_key,        timeout=timeout,        blocking_timeout=timeout + 5,    )    acquired = False    try:        acquired = lock.acquire(blocking=True)        if not acquired:            raise Exception(f&quot;[get_redis_lock] Failed to acquire lock: &#123;lock_key&#125;&quot;)        yield lock    finally:        if acquired:            try:                lock.release()                logger.info(f&quot;[get_redis_lock] Released lock: &#123;lock_key&#125;&quot;)            except Exception as e:                logger.warning(f&quot;Lock release error: &#123;e&#125;&quot;)\n\n调用时非常优雅：\n@app.post(&quot;/seckill&quot;)def seckill():    with get_redis_lock(&quot;lock:inventory&quot;, timeout=5):        stock = int(redis_client.get(&quot;stock&quot;) or 0)        if stock &lt;= 0:            raise HTTPException(status_code=400, detail=&quot;库存不足&quot;)        redis_client.decr(&quot;stock&quot;)        return &#123;&quot;msg&quot;: &quot;秒杀成功&quot;&#125;\n\n👉 有了 with，即使代码抛异常，也能保证锁被正确释放。\n\n🧪 多副本 + 多线程脚本场景分布式锁不仅能保护接口，还能用在 后台定时任务。\n比如：多个 FastAPI 副本同时启动了一个定时脚本去生成报表，如果没加锁，结果就是——每个副本都生成一次报表，浪费资源还可能产生重复数据。\n有了 get_redis_lock：\nasync def generate_report():    with get_redis_lock(&quot;lock:report&quot;, timeout=30):        print(&quot;开始生成报表...&quot;)        await asyncio.sleep(5)  # 模拟耗时任务        print(&quot;报表生成完成 ✅&quot;)\n\n这也适用于多副本的消费者脚本场景：例如高并发情况下，多个消费者同时向数据库插入数据，虽然插入之前都会进行检查数据是否存在，但还是会出现重复数据。这个时候就需要用到分布式锁来保证数据的唯一性。\n即使部署了多个副本，也只有一个实例能拿到锁执行，避免重复任务。\n\n⚠️ 踩坑与优化\n锁过期时间太短\n\n业务没执行完，锁就过期，其他实例进来重复执行。\n✅ 解决：加“锁续期”（看门狗）。\n\n\n锁过期时间太长\n\n服务宕机后锁迟迟不释放，影响可用性。\n\n\n高并发下锁冲突严重\n\n可以结合 消息队列 来削峰填谷。\n\n\n\n\n🏁 总结那次“库存超卖”事故后，我对分布式锁有了更深的认识：\n\n在 单机时代，threading.Lock 足够。\n在 分布式环境下，Redis 分布式锁是最常用的解决方案。\n在 FastAPI 项目里，配合 @contextmanager 写法，能让锁的获取与释放更安全、优雅。\n\n一句话：\n\n分布式锁不是银弹，但在关键场景下，它是救命稻草。\n\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.160-fastapi中请求唯一标识的使用","url":"/2025/09/26/No-160-fastapi%E4%B8%AD%E8%AF%B7%E6%B1%82%E5%94%AF%E4%B8%80%E6%A0%87%E8%AF%86%E7%9A%84%E4%BD%BF%E7%94%A8/","content":"导读在项目排查问题的过程中，链路跟踪是一个很重要的手段，通过链路跟踪，可以快速定位问题，从而快速解决问题。\n但是当没有链路跟踪的时候，我们也可以通过请求唯一标识，来追踪当前项目请求日志，从而定位问题。\n至少在本项目中能一次性全部搜索出来当前请求的日志，快速查看运行逻辑是否正确。\nfastapi 中请求唯一标识的使用添加一个中间件，将请求的唯一标识添加到请求头中。\nimport loggingfrom starlette.middleware.base import BaseHTTPMiddlewarefrom fastapi import Requestimport uuidfrom app.core.logger import loggerfrom app.core.logger import request_id_ctxclass RequestIDMiddleware(BaseHTTPMiddleware):    async def dispatch(self, request: Request, call_next):        # 获取或生成请求ID        request_id = request.headers.get(&#x27;X-Request-ID&#x27;) or str(uuid.uuid4())        request.state.request_id = request_id                # 设置上下文变量        token = request_id_ctx.set(request_id)                try:            logger.info(f&quot;Request started: &#123;request.method&#125; &#123;request.url&#125;&quot;)            response = await call_next(request)            response.headers[&#x27;X-Request-ID&#x27;] = request_id            logger.info(f&quot;Request completed: &#123;request.method&#125; &#123;request.url&#125; &#123;response.status_code&#125;&quot;)            return response        except Exception as e:            logger.error(f&quot;Request failed: &#123;request.method&#125; &#123;request.url&#125; &#123;str(e)&#125;&quot;)            raise        finally:            request_id_ctx.reset(token)\n\n主程序中添加中间件app = FastAPI(    title=settings.PROJECT_NAME,    openapi_url=f&quot;/openapi.json&quot;,    generate_unique_id_function=custom_generate_unique_id,)# 请求ID中间件app.add_middleware(RequestIDMiddleware)\n\n日志中添加请求IDimport loggingfrom concurrent_log_handler import ConcurrentRotatingFileHandlerfrom app.core.config import settingsfrom contextvars import ContextVar# 创建上下文变量存储请求IDrequest_id_ctx = ContextVar(&quot;request_id&quot;, default=None)class RequestIDFilter(logging.Filter):    def filter(self, record):        # 从当前请求上下文中获取请求ID        record.request_id = request_id_ctx.get() or &quot;no-request-id&quot;        return True\n\n设置日志格式# 设置日志的格式，可以根据需要修改formatter = logging.Formatter(    &quot;%(asctime)s &quot;    &quot;%(filename)s:%(lineno)d &quot;    &quot;%(processName)s &quot;    &quot;%(threadName)s &quot;    &quot;%(levelname)s &quot;    &quot;%(request_id)s &quot;    &quot;%(message)s&quot;)\n\n添加日志处理器logger.addFilter(RequestIDFilter())\n\n使用logger.info(&quot;This is an info message with request ID&quot;)\n\n在日志中可以看到请求ID:\n2025-09-19 18:33:00,354 middleware.py:22 MainProcess Thread-1 (run_blocking_portal) INFO 870b229f-1546-4147-873a-f3763b548db7 Request completed: POST http://testserver/coding_callback_manager/issue_status_change_first/1.0.0 5002025-09-19 18:33:54,885 middleware.py:19 MainProcess Thread-1 (run_blocking_portal) INFO f8cf2dfc-79ff-43dc-a384-f4a51c3f52c0 Request started: POST http://testserver/coding_callback_manager/issue_status_change_first/1.0.0\n\n在返回请求头中，也可以看到请求ID:\nContent-Type: application/jsonContent-Length: 66Connection: closeServer: openrestyDate: Thu, 25 Sep 2025 17:11:44 GMTX-Proxy-By: Gin Reverse ProxyX-Request-Id: 3ab700b0-b818-4d22-8e91-671ea46317e2\n\n总结\n通过请求唯一标识，可以快速定位请求的链路，从而定位问题。\n通过请求唯一标识能快速搜索出来当前请求的日志。\n可以从返回请求头中拿到请求ID，再去搜索日志。\n可以通过请求唯一 id，查询到请求开始时间和结束时间，分析请求耗时。\n可以通过逐步打印日志，来分析请求的流程，分析性能瓶颈。\n\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.153-fastapi中使用redis-pool连接池","url":"/2025/08/12/No-153-fastapi%E4%B8%AD%E4%BD%BF%E7%94%A8redispool%E8%BF%9E%E6%8E%A5%E6%B1%A0/","content":"导读在实际开发过程中，为了提升接口性能，我们经常会选择使用数据库连接池、redis 连接池等。\n通过连接池连接，提升效率，节省内存开销。同时通过将数据缓存到 redis 中，还可以减少网络 IO 开销。\n通常一个接口直接跟数据库交互，例如 MySQL，响应时间大概在 200ms ~ 500ms 左右。加上 redis 缓存之后，接口直接从缓存中读取，响应时间可以缩短到 10ms 左右，可以说是质的飞跃。\n下面将介绍如何在 FastAPI 中使用 redis 连接池。\n1、安装库首先确保安装了必要的 Python 包：\npip install fastapi redis uvicorn\n\n2. 添加配置在根目录下，添加 .env.dev 文件，里面是项目相关配置。\nREDIS_HOST=&quot;192.168.201.200&quot;REDIS_PORT=&quot;6379&quot;REDIS_PASSWORD=&quot;&lt;PASSWORD&gt;&quot;REDIS_DB=&quot;9&quot;\n\n在 config.py 文件中，增加如下代码，读取配置信息。\nimport osfrom typing import Annotated, Anyfrom pydantic import (    AnyUrl,    BeforeValidator,    PostgresDsn,    computed_field,)from pydantic_core import MultiHostUrlfrom pydantic_settings import BaseSettings, SettingsConfigDictclass Settings(BaseSettings):    if os.getenv(&quot;APP_ENV&quot;) == &quot;development&quot; or not os.getenv(&quot;APP_ENV&quot;):            model_config = SettingsConfigDict(                env_file=&quot;.env.dev&quot;, env_ignore_empty=True, extra=&quot;ignore&quot;            )    if os.getenv(&quot;APP_ENV&quot;) == &quot;production&quot;:        model_config = SettingsConfigDict(            env_file=&quot;/app/.env.production&quot;, env_ignore_empty=True, extra=&quot;ignore&quot;        )    if os.getenv(&quot;APP_ENV&quot;) == &quot;test&quot;:        model_config = SettingsConfigDict(            env_file=&quot;.env.test&quot;, env_ignore_empty=True, extra=&quot;ignore&quot;        )    REDIS_HOST: str    REDIS_PORT: int     REDIS_PASSWD: str     REDIS_DB: intsettings = Settings()\n\n3. 添加 redis 连接池在 db.py 中，创建 redis 连接池。\nfrom config import settingsimport redisfrom fastapi import Depends# 创建 Redis 连接池redis_pool = redis.ConnectionPool(    host=settings.REDIS_HOST,    port=settings.REDIS_PORT,    db=settings.REDIS_DB,    decode_responses=True,  # 自动解码返回的字节为字符串    max_connections=10     # 最大连接数)\n\n4. 创建deps在 deps.py 文件中，定义一个依赖项。\nimport redisfrom logger import loggerdef get_redis():    &quot;&quot;&quot;获取 Redis 连接的依赖函数&quot;&quot;&quot;    try:        redis_conn = redis.Redis(connection_pool=redis_pool)        yield redis_conn    except Exception as e:        # 可选择加入日志记录或错误上报        logger.exception(f&quot;Error occurred while managing redis pool session: &#123;e&#125;&quot;)        raise    finally:        # 这里不需要手动关闭连接，连接会返回到连接池中        passRedisPoolDep = Annotated[Redis, Depends(get_redis)]\n\n5. 使用 RedisPoolDep 依赖项在你的主应用文件（如 main.py）中使用这个连接池：\nfrom fastapi import FastAPI, Dependsfrom deps import RedisPoolDepimport redisapp = FastAPI()@app.get(&quot;/set/&#123;key&#125;/&#123;value&#125;&quot;)async def set_key_value(    key: str,     value: str,     redis_conn: RedisPoolDep):    &quot;&quot;&quot;设置键值对&quot;&quot;&quot;    redis_conn.set(key, value)    return &#123;&quot;message&quot;: f&quot;Set &#123;key&#125; = &#123;value&#125;&quot;&#125;@app.get(&quot;/get/&#123;key&#125;&quot;)async def get_key(    key: str,     redis_conn: RedisPoolDep):    &quot;&quot;&quot;获取键值&quot;&quot;&quot;    value = redis_conn.get(key)    return &#123;&quot;key&quot;: key, &quot;value&quot;: value&#125;@app.get(&quot;/info&quot;)async def redis_info(redis_conn: RedisPoolDep):    &quot;&quot;&quot;获取 Redis 服务器信息&quot;&quot;&quot;    info = redis_conn.info()    return &#123;&quot;redis_info&quot;: info&#125;\n\n总结\n创建 Redis 连接池并在应用启动时初始化\n创建依赖函数 get_redis() 来获取连接\n在路由处理函数中使用 Depends(get_redis) 注入 Redis 连接\n对于更复杂的应用，将业务逻辑封装到服务层\n考虑使用异步 Redis 客户端以提高性能, 例如 aioredis。这里有坑，慎用。。。\n使用环境变量管理配置\n\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.157-fastapi中如何生成swagger文档","url":"/2025/08/25/No-157-fastapi%E4%B8%AD%E5%A6%82%E4%BD%95%E7%94%9F%E6%88%90swagger%E6%96%87%E6%A1%A3/","content":"导读fastapi 项目开始了，怎么快速定义 swagger 文档，供测试、前端查看呢？本文将介绍如何使用 fastapi 生成 swagger 文档。\n生成swagger文档其实，fastapi 本身就支持生成 swagger 文档，当我们使用 uvicorn 启动项目时，只需要在浏览器中输入 http://127.0.0.1:8000/docs 即可看到 swagger 文档。\n生成文档非常简单，但是标准化的文档比较难，往往需要预定好一些规则，比如参数的类型、参数的描述、参数的默认值等。如果这些规则没有统一，那么生成的文档将非常混乱，难以阅读。\n幸好，fastapi 提供了 pydantic 模块，既能用来校验接口的输入和输出，又能用来生成 swagger 文档。\npydantic使用可以参考 No.148-fastapi中使用标准化出入参自动化生成接口文档。\n生成文档直接访问 http://127.0.0.1:8000/docs 即可看到 swagger 文档。\n但是当我们需要将文档导入到第三方平台的时候，比如 postman，就需要将文档导出为 json 格式。\n可以使用命令行快速生成 json 格式的文档文件：\n# 启动项目fastapi dev --port 9000 --reload# 生成文档curl -X GET http://127.0.0.1:9000/openapi.json &gt; docs/api/swagger.json\n\n生成的文档将在 docs/api/swagger.json 文件中。再手动导入到相关平台即可。或者使用 openAPI 编写脚本自动化导入。\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.158-当我弃用pycharm改用vscode之后","url":"/2025/09/01/No-158-%E5%BD%93%E6%88%91%E5%BC%83%E7%94%A8pycharm%E6%94%B9%E7%94%A8vscode%E4%B9%8B%E5%90%8E/","content":"导读作为一名Python开发者，我使用PyCharm已经超过五年了。它无疑是一款强大的IDE，智能提示、调试功能、项目导航都无可挑剔。但随着时间的推移，一些不适开始浮现：\n内存占用惊人：打开大型项目时，PyCharm动不动就吃掉4GB+内存，我的16GB笔记本开始捉襟见肘。尤其是使用AI 插件后，内存占用更是飙升。代码补全显得很卡顿，严重影响编码体验。而且我 256GB 的 SSD 硬盘也吃不消了。\n启动速度慢：从点击图标到真正开始编码，等待时间比较久，打开 Pycharm，一般要去个卫生间再回来开始工作会比较顺畅。\n过于“重量级”：有时候我只是想快速编辑一个小脚本，却需要启动整个IDE。单个文件编辑，其实 sublime text 3 就足够了。\n尝试转变：初体验VSCode一开始我是怀疑的——一个“编辑器”真能替代“IDE”吗？\n安装必要的Python扩展后，我惊讶地发现VSCode提供了绝大多数我需要的功能：\n\n智能提示：通过Pylance扩展，代码补全几乎不输PyCharm\n调试支持：完整的断点调试、变量监视功能\n虚拟环境支持：轻松切换不同Python解释器\nGit集成：源代码管理直观易用\n\n绝了绝了。\n我也是尝试了很多次，切换到VSCode后，感觉不顺再切换回来。\n这样尝试了 7～8 次，我终于决定彻底抛弃PyCharm，拥抱VSCode。\n惊喜发现：VSCode的独特优势使用几周后，我发现了VSCode许多令人欣喜的特点：\n闪电般的启动速度：几乎瞬间启动，随时记录灵感不再是问题\n内存友好：通常内存占用只有PyCharm的1&#x2F;3到1&#x2F;2\n无缝多语言支持：前端项目常常需要同时处理Python、JavaScript、HTML，VSCode天生就是多语言环境\n扩展生态丰富：几乎任何需求都能找到对应的扩展，且安装管理极其简单\n高度可定制：通过settings.json可以精细调整每一个细节\n适应挑战：需要克服的障碍转变并非一帆风顺，确实有一些需要适应的地方：\n项目管理方式不同：VSCode以文件夹为基础而不是“项目”概念，需要时间适应\n部分高级功能需要配置：一些PyCharm开箱即用的功能，在VSCode中需要额外设置\n重构工具略逊一筹：虽然可用，但PyCharm的重构工具确实更强大\n我的VSCode Python开发配置经过不断调整，这是我的黄金配置：\n&#123;  &quot;python.linting.pylintEnabled&quot;: true,  &quot;python.linting.enabled&quot;: true,  &quot;python.formatting.provider&quot;: &quot;black&quot;,  &quot;editor.formatOnSave&quot;: true,  &quot;python.linting.pylintArgs&quot;: [&quot;--load-plugins&quot;, &quot;pylint_django&quot;],  &quot;python.linting.flake8Enabled&quot;: false,  &quot;[python]&quot;: &#123;    &quot;editor.defaultFormatter&quot;: &quot;ms-python.python&quot;,    &quot;editor.tabSize&quot;: 4  &#125;&#125;\n\n必备扩展：\n\nPython (Microsoft)\nPylance\nJupyter\nPython Docstring Generator\nBlack Formatter\nMagicPython\nPython Debug (Microsoft)\n\n结论：适合的才是最好的经过几个月的使用，我确定VSCode已经满足了我95%的Python开发需求。剩下的5%主要是超大型项目的重构和深度数据库集成，这些场景下我仍然会打开PyCharm。\n适合VSCode的场景：\n\nWeb开发（尤其是全栈开发）\n数据科学和脚本编写\n快速编辑和轻量级项目\n需要频繁切换语言的项目\n\n可能仍需PyCharm的场景：\n\n超大型项目导航和重构\n复杂的数据库集成开发\n需要IDE级深度集成的企业环境\n\n转变编辑器不是非此即彼的选择，而是找到最适合自己工作流程的工具。对我来说，VSCode提供了更好的平衡点：既强大又灵活，既功能丰富又保持轻量。\n也许有一天我会再次改变我的工具选择，但至少现在，VSCode让我享受编码的乐趣变得更加简单直接。\n安利一波：VSCode 的搜索功能是真的好用。\n\n你有类似的工具转变经历吗？欢迎在评论区分享你的故事！\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["fastapi"]},{"title":"No.162-fastapi中使用 celery.backend_cleanup 的工作原理","url":"/2025/10/31/No-162-fastapi%E4%B8%AD%E4%BD%BF%E7%94%A8celery.backend_cleanup%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/","content":"导读FastApi 中使用定时任务和异步任务，首选 Celery。在启动 Celery Beat 的时候，会自动初始化一个 celery.backend_cleanup 任务。那么这个任务做了哪些事情呢？工作原理是什么？怎么使用和配置呢？\n作用它的作用：\n\n遍历 result backend；\n找出超过 result_expires 时间的任务结果；\n将其删除。\n\n关键配置项：\n\n\n\n配置项\n说明\n示例\n\n\n\nresult_expires\n任务结果保存的过期时间（秒）\napp.conf.result_expires = 3600（1小时）\n\n\nresult_backend\n存储任务结果的后端\nredis://localhost:6379/0\n\n\nresult_backend_cleanup_interval（Celery 5.3+）\n清理任务运行间隔\napp.conf.result_backend_cleanup_interval = 3600\n\n\n举例配置：\napp.conf.update(    result_backend=&quot;redis://localhost:6379/0&quot;,    result_expires=3600,  # 一小时，非常重要，它决定了你的任务跑完存储的时长)\n\n命令行触发：\ncelery -A your_project call celery.backend_cleanup\n\npython调用：\nfrom celery import Celeryapp = Celery(&#x27;your_project&#x27;)app.send_task(&#x27;celery.backend_cleanup&#x27;)\n\n如果你用的 Beat scheduler 为数据库，则会创建一条定时任务数据，通常是 0 4 * * *，表示在每天的凌晨 4 点执行。\n注意事项：\n\n如果你关闭了 result_persistent&#x3D;False 或根本没用 backend（比如只 fire-and-forget），则无需关心。\n如果你用的是 数据库 作为 backend（如 Django ORM、SQLAlchemy），清理是删除数据库记录；\n如果你用的是 Redis，则清理是删除对应的 key；\n对于自定义 backend，必须实现 cleanup() 方法，否则该任务无效。\n\n工作原理\n首先判断结果数据是否过期\n再根据筛选出来的过期数据，进行删除\n\n判断过期数据的逻辑：Celery 通过比较 任务结果的存储时间 + result_expires 来判断是否过期。简化逻辑如下：\nif now() - date_done &gt; result_expires:    # 任务结果已过期，删除\n\n不同 Backend 的判断方式：\n\n\n\nBackend 类型\n过期判断方式\n说明\n\n\n\nRedis &#x2F; Memcached\n利用 key 的 TTL（过期时间）\nCelery 在写入结果时会设置 EXPIRE，Redis 自动清除\n\n\nDatabase（如 Django ORM、SQLAlchemy）\n读取 date_done 字段，对比当前时间\nCelery 定期执行 DELETE FROM celery_taskmeta WHERE date_done &lt; now() - result_expires\n\n\nRPC &#x2F; AMQP\n不持久化结果或自动过期\n一般不需清理\n\n\nFilesystem &#x2F; S3 等持久化存储\n检查保存文件的时间戳\n比对 mtime 或 metadata\n\n\nCelery.backend_cleanup\n自动清理过期数据\nCelery 定期执行 backend_cleanup，清理过期数据\n\n\n以数据库 backend 为例：在 Celery 的源码中（celery&#x2F;backends&#x2F;database&#x2F;init.py），DatabaseBackend 有一个 cleanup() 方法：\ndef cleanup(self):    session = self.ResultSession()    now = self.app.now()    expired = now - self.expires    session.query(TaskModel).filter(TaskModel.date_done &lt; expired).delete()    session.commit()\n\n过期数据常见的存储方式：\n\n\n\nBackend\n存储位置\n示例\n\n\n\nRedis\nRedis 的 key-value\nkey 类似于 celery-task-meta-&lt;uuid&gt;\n\n\nDatabase（Django ORM &#x2F; SQLAlchemy）\n数据库表中（默认表名：celery_taskmeta）\n列：id, task_id, status, result, date_done, traceback\n\n\nAMQP (RabbitMQ)\n消息队列，不持久化\n任务完成后即消费\n\n\nFilesystem\n文件系统的结果文件\n每个任务结果单独存文件\n\n\nCache backend (memcached)\n缓存中\n自动过期\n\n\nS3 &#x2F; HTTP\n远程对象存储\n带 metadata 标识时间戳\n\n\n配置和使用要使用它，需要在 celery 配置文件中添加以下配置：\n# celeryconfig.pyCELERY_RESULT_BACKEND = &#x27;redis://localhost:6379/0&#x27;CELERY_RESULT_EXPIRES = 3600  # 1 hour   过期时间，单位为秒，存储够小时就删除CELERY_RESULT_CLEANUP_INTERVAL = 60  # 1 minute，每分钟执行一次清理任务，过期任务会被自动删除\n\n\n每日踩一坑，生活更轻松。\n本期分享就到这里啦，祝君在测开之路上越走越顺，越走越远。\n","tags":["Celery"]}]